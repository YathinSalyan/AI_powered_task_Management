{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61205717-26f1-47f0-a190-d80830b1e1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded NLTK resource: punkt\n",
      "Successfully downloaded NLTK resource: stopwords\n",
      "Successfully downloaded NLTK resource: wordnet\n",
      "Using dataset from: /Users/krishnashetty/Desktop/task_manger/Twitter_Data.csv\n",
      "Loading dataset from /Users/krishnashetty/Desktop/task_manger/Twitter_Data.csv...\n",
      "Dataset shape: (162980, 2)\n",
      "\n",
      "Sample data:\n",
      "                                          clean_text  category\n",
      "0  when modi promised â€œminimum government maximum...      -1.0\n",
      "1  talk all the nonsense and continue all the dra...       0.0\n",
      "2  what did just say vote for modi  welcome bjp t...       1.0\n",
      "3  asking his supporters prefix chowkidar their n...       1.0\n",
      "4  answer who among these the most powerful world...       1.0\n",
      "\n",
      "Original sentiment values: [-1.  0.  1.]\n",
      "Mapped sentiment values: [0 1]\n",
      "Sentiment value counts after mapping:\n",
      "sentiment_binary\n",
      "0    90720\n",
      "1    72249\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Preprocessing text data...\n",
      "\n",
      "Final sentiment distribution:\n",
      "sentiment_binary\n",
      "0    90651\n",
      "1    72244\n",
      "Name: count, dtype: int64\n",
      "Extracting features using TF-IDF...\n",
      "Training logistic regression model...\n",
      "\n",
      "Accuracy: 0.8997513735842106\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91     18130\n",
      "           1       0.92      0.85      0.88     14449\n",
      "\n",
      "    accuracy                           0.90     32579\n",
      "   macro avg       0.90      0.89      0.90     32579\n",
      "weighted avg       0.90      0.90      0.90     32579\n",
      "\n",
      "Error saving models: Can't pickle local object 'create_task_optimization_model.<locals>.optimize_tasks'\n",
      "\n",
      "Testing sentiment analysis on sample task descriptions:\n",
      "Text: 'This task is urgent and needs immediate attention'\n",
      "Predicted sentiment: Negative (confidence: 0.84)\n",
      "\n",
      "Text: 'I'm looking forward to working on this interesting project'\n",
      "Predicted sentiment: Positive (confidence: 0.93)\n",
      "\n",
      "Text: 'This is a low priority task that can be done anytime'\n",
      "Predicted sentiment: Negative (confidence: 0.85)\n",
      "\n",
      "Text: 'This task is frustrating and difficult to complete'\n",
      "Predicted sentiment: Negative (confidence: 0.73)\n",
      "\n",
      "\n",
      "Model development completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis Model Development for Task Management System\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a directory for saving models if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Download necessary NLTK resources with proper error handling\n",
    "def download_nltk_resources():\n",
    "    resources = ['punkt', 'stopwords', 'wordnet']\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            nltk.download(resource, quiet=True)\n",
    "            print(f\"Successfully downloaded NLTK resource: {resource}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading NLTK resource {resource}: {e}\")\n",
    "            print(\"Attempting to continue without this resource...\")\n",
    "\n",
    "download_nltk_resources()\n",
    "\n",
    "# Define robust text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text for sentiment analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The input text to preprocess\n",
    "    \n",
    "    Returns:\n",
    "    str: The preprocessed text\n",
    "    \"\"\"\n",
    "    # Handle NaN, None, or non-string values\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove user @ references and '#'\n",
    "        text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "        \n",
    "        # Remove punctuations and numbers\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Tokenize the text\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "        except LookupError:\n",
    "            # Fallback if NLTK resources are not available\n",
    "            tokens = text.split()\n",
    "        \n",
    "        # Remove stopwords if available\n",
    "        try:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = [word for word in tokens if word not in stop_words]\n",
    "        except LookupError:\n",
    "            # Fallback if stopwords are not available\n",
    "            print(\"Stopwords not available, skipping stopword removal\")\n",
    "        \n",
    "        # Lemmatize if available\n",
    "        try:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        except LookupError:\n",
    "            # Fallback if lemmatizer is not available\n",
    "            print(\"WordNet lemmatizer not available, skipping lemmatization\")\n",
    "        \n",
    "        # Join tokens back into string\n",
    "        text = ' '.join(tokens)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing text: {e}\")\n",
    "        return text  # Return original text if preprocessing fails\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads and prepares the dataset for sentiment analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X, y) preprocessed features and target variables\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load dataset\n",
    "        print(f\"Loading dataset from {file_path}...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(\"Dataset shape:\", df.shape)\n",
    "        print(\"\\nSample data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # For this specific dataset structure\n",
    "        text_column = 'clean_text'\n",
    "        sentiment_column = 'category'\n",
    "        \n",
    "        # Verify required columns exist\n",
    "        if text_column not in df.columns:\n",
    "            # Try to find a column that might contain text\n",
    "            text_cols = [col for col in df.columns if any(x in col.lower() for x in ['text', 'content', 'message', 'tweet'])]\n",
    "            if text_cols:\n",
    "                print(f\"'{text_column}' column not found. Using '{text_cols[0]}' as text column.\")\n",
    "                text_column = text_cols[0]\n",
    "            else:\n",
    "                raise ValueError(\"Could not find a text column in the dataset\")\n",
    "                \n",
    "        # Rename for consistency\n",
    "        df['text'] = df[text_column]\n",
    "        \n",
    "        # Verify sentiment column exists\n",
    "        if sentiment_column not in df.columns:\n",
    "            # Try to find a column that might contain sentiment\n",
    "            sentiment_cols = [col for col in df.columns if any(x in col.lower() for x in ['sentiment', 'label', 'class', 'target', 'polarity', 'category'])]\n",
    "            if sentiment_cols:\n",
    "                print(f\"'{sentiment_column}' column not found. Using '{sentiment_cols[0]}' as sentiment column.\")\n",
    "                sentiment_column = sentiment_cols[0]\n",
    "            else:\n",
    "                raise ValueError(\"Could not find a sentiment column in the dataset\")\n",
    "        \n",
    "        # Rename for consistency\n",
    "        df['sentiment'] = df[sentiment_column]\n",
    "            \n",
    "        # Drop rows with missing values in critical columns\n",
    "        df = df.dropna(subset=['text', 'sentiment'])\n",
    "        \n",
    "        # Map sentiment values from the dataset's format (-1, 0, 1) to binary classification (0, 1)\n",
    "        # Assuming -1 and 0 are negative, 1 is positive\n",
    "        print(\"\\nOriginal sentiment values:\", df['sentiment'].unique())\n",
    "        \n",
    "        # Map values: consider only positive (1.0) as 1, all others (0.0, -1.0) as 0\n",
    "        df['sentiment_binary'] = df['sentiment'].apply(lambda x: 1 if x == 1.0 else 0)\n",
    "        \n",
    "        # Verify the mapping worked\n",
    "        print(\"Mapped sentiment values:\", df['sentiment_binary'].unique())\n",
    "        print(\"Sentiment value counts after mapping:\")\n",
    "        print(df['sentiment_binary'].value_counts())\n",
    "        \n",
    "        # Preprocess the text data\n",
    "        print(\"\\nPreprocessing text data...\")\n",
    "        df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "        \n",
    "        # Remove rows where preprocessing failed (empty string)\n",
    "        df = df[df['processed_text'].str.strip() != '']\n",
    "        \n",
    "        # Display data distribution\n",
    "        print(\"\\nFinal sentiment distribution:\")\n",
    "        print(df['sentiment_binary'].value_counts())\n",
    "        \n",
    "        return df['processed_text'], df['sentiment_binary']\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading and preparing data: {e}\")\n",
    "        raise\n",
    "\n",
    "def train_sentiment_model(X, y):\n",
    "    \"\"\"\n",
    "    Trains a sentiment analysis model.\n",
    "    \n",
    "    Parameters:\n",
    "    X: Features (processed text)\n",
    "    y: Target variable (sentiment)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model, vectorizer, evaluation_metrics)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split the dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        # Feature extraction using TF-IDF\n",
    "        print(\"Extracting features using TF-IDF...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "        \n",
    "        # Train a logistic regression model\n",
    "        print(\"Training logistic regression model...\")\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        print(\"\\nAccuracy:\", accuracy)\n",
    "        print(\"\\nClassification Report:\\n\", class_report)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig('models/confusion_matrix.png')  # Save the figure\n",
    "        plt.close()  # Close the figure to free memory\n",
    "        \n",
    "        return model, tfidf_vectorizer, {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': class_report,\n",
    "            'confusion_matrix': conf_matrix\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error training sentiment model: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_task_optimization_model():\n",
    "    \"\"\"\n",
    "    Creates a task optimization model that scores tasks\n",
    "    based on sentiment, deadline proximity, and assigned priority.\n",
    "    \n",
    "    Returns:\n",
    "    function: Task optimization function\n",
    "    \"\"\"\n",
    "    \n",
    "    def optimize_tasks(tasks_df):\n",
    "        \"\"\"\n",
    "        Optimize and prioritize tasks based on multiple factors.\n",
    "        \n",
    "        Parameters:\n",
    "        tasks_df (DataFrame): DataFrame containing task information\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: Tasks sorted by optimization score\n",
    "        \"\"\"\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        df = tasks_df.copy()\n",
    "        \n",
    "        # Calculate days to deadline\n",
    "        if 'deadline' in df.columns:\n",
    "            try:\n",
    "                df['days_to_deadline'] = pd.to_datetime(df['deadline'], errors='coerce') - pd.Timestamp.now()\n",
    "                df['days_to_deadline'] = df['days_to_deadline'].dt.total_seconds() / (24 * 3600)\n",
    "                \n",
    "                # Handle missing or invalid dates\n",
    "                df['days_to_deadline'] = df['days_to_deadline'].fillna(30)  # Default to 30 days\n",
    "                \n",
    "                # Normalize deadline score (closer deadline = higher score)\n",
    "                max_days = max(df['days_to_deadline'].max(), 30)  # At least 30 days to avoid division by zero\n",
    "                df['deadline_score'] = 1 - (df['days_to_deadline'] / max_days).clip(0, 1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating deadline score: {e}\")\n",
    "                df['deadline_score'] = 0.5  # Default score\n",
    "        else:\n",
    "            df['deadline_score'] = 0.5  # Default score\n",
    "            \n",
    "        # Convert priority to numeric score\n",
    "        priority_map = {'urgent': 1.0, 'high': 0.75, 'medium': 0.5, 'low': 0.25}\n",
    "        if 'priority' in df.columns:\n",
    "            df['priority_score'] = df['priority'].str.lower().map(priority_map).fillna(0.5)\n",
    "        else:\n",
    "            df['priority_score'] = 0.5  # Default score\n",
    "            \n",
    "        # Add sentiment score if available\n",
    "        if 'sentiment_score' not in df.columns:\n",
    "            df['sentiment_score'] = 0.5  # Default score\n",
    "            \n",
    "        # Calculate final optimization score\n",
    "        df['optimization_score'] = (\n",
    "            df['deadline_score'] * 0.4 +  # Weight for deadline\n",
    "            df['priority_score'] * 0.4 +  # Weight for priority\n",
    "            df['sentiment_score'] * 0.2    # Weight for sentiment\n",
    "        )\n",
    "        \n",
    "        # Sort tasks by optimization score\n",
    "        return df.sort_values('optimization_score', ascending=False)\n",
    "    \n",
    "    return optimize_tasks\n",
    "\n",
    "def create_predictive_analytics_model():\n",
    "    \"\"\"\n",
    "    Creates a predictive analytics model for estimating task completion time.\n",
    "    \n",
    "    Returns:\n",
    "    function: Task completion time prediction function\n",
    "    \"\"\"\n",
    "    \n",
    "    def predict_completion_time(task_history_df, new_task):\n",
    "        \"\"\"\n",
    "        Predicts task completion time based on historical data\n",
    "        \n",
    "        Parameters:\n",
    "        task_history_df: DataFrame with columns 'task_type', 'priority', 'assigned_to', 'actual_completion_time'\n",
    "        new_task: dict with 'task_type', 'priority', 'assigned_to'\n",
    "        \n",
    "        Returns:\n",
    "        float: Estimated hours to complete the task\n",
    "        \"\"\"\n",
    "        if task_history_df is None or task_history_df.empty:\n",
    "            # Default predictions if no history\n",
    "            base_times = {'bug_fix': 4, 'feature': 8, 'documentation': 2, 'other': 6}\n",
    "            priority_multipliers = {'urgent': 0.8, 'high': 0.9, 'medium': 1.0, 'low': 1.2}\n",
    "            \n",
    "            task_type = new_task.get('task_type', 'other').lower()\n",
    "            priority = new_task.get('priority', 'medium').lower()\n",
    "            \n",
    "            base_time = base_times.get(task_type, 6)\n",
    "            multiplier = priority_multipliers.get(priority, 1.0)\n",
    "            \n",
    "            return base_time * multiplier\n",
    "        \n",
    "        try:\n",
    "            # Filter relevant history\n",
    "            filtered_df = task_history_df.copy()\n",
    "            \n",
    "            # Handle potential missing columns\n",
    "            required_columns = ['task_type', 'priority', 'assigned_to', 'actual_completion_time']\n",
    "            for column in required_columns:\n",
    "                if column not in filtered_df.columns:\n",
    "                    print(f\"Warning: '{column}' column not found in task history\")\n",
    "                    if column == 'actual_completion_time':\n",
    "                        # This is critical - we can't continue without it\n",
    "                        raise ValueError(\"'actual_completion_time' column is required for prediction\")\n",
    "            \n",
    "            # Apply filters if corresponding columns exist\n",
    "            if 'task_type' in filtered_df.columns and 'task_type' in new_task:\n",
    "                filtered_df = filtered_df[filtered_df['task_type'].str.lower() == new_task['task_type'].lower()]\n",
    "            \n",
    "            if 'priority' in filtered_df.columns and 'priority' in new_task:\n",
    "                filtered_df = filtered_df[filtered_df['priority'].str.lower() == new_task['priority'].lower()]\n",
    "                \n",
    "            if 'assigned_to' in filtered_df.columns and 'assigned_to' in new_task:\n",
    "                filtered_df = filtered_df[filtered_df['assigned_to'] == new_task['assigned_to']]\n",
    "            \n",
    "            # Calculate average completion time\n",
    "            if filtered_df.empty:\n",
    "                # Fall back to overall average if no matching tasks\n",
    "                return task_history_df['actual_completion_time'].mean()\n",
    "            else:\n",
    "                return filtered_df['actual_completion_time'].mean()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting completion time: {e}\")\n",
    "            # Return a reasonable default\n",
    "            return 4.0  # Default 4 hours\n",
    "    \n",
    "    return predict_completion_time\n",
    "\n",
    "def predict_sentiment(text, model, vectorizer):\n",
    "    \"\"\"\n",
    "    Predicts sentiment of a given text.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The text to analyze\n",
    "    model: Trained sentiment model\n",
    "    vectorizer: TF-IDF vectorizer\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (sentiment, confidence)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        processed = preprocess_text(text)\n",
    "        if not processed:  # If preprocessing resulted in empty string\n",
    "            return \"Neutral\", 0.5\n",
    "            \n",
    "        tfidf_vector = vectorizer.transform([processed])\n",
    "        prediction = model.predict(tfidf_vector)[0]\n",
    "        probability = model.predict_proba(tfidf_vector)[0]\n",
    "        \n",
    "        if prediction == 1:\n",
    "            sentiment = \"Positive\"\n",
    "            confidence = probability[1]\n",
    "        else:\n",
    "            sentiment = \"Negative\"\n",
    "            confidence = probability[0]\n",
    "            \n",
    "        return sentiment, confidence\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting sentiment: {e}\")\n",
    "        return \"Neutral\", 0.5\n",
    "\n",
    "def save_models(model, vectorizer, task_optimizer, predictive_model):\n",
    "    \"\"\"\n",
    "    Saves all models to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained sentiment model\n",
    "    vectorizer: TF-IDF vectorizer\n",
    "    task_optimizer: Task optimization function\n",
    "    predictive_model: Predictive analytics function\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save the sentiment model\n",
    "        with open('models/sentiment_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "        # Save the vectorizer\n",
    "        with open('models/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "            pickle.dump(vectorizer, f)\n",
    "        \n",
    "        # Save the task optimizer\n",
    "        with open('models/task_optimizer.pkl', 'wb') as f:\n",
    "            pickle.dump(task_optimizer, f)\n",
    "        \n",
    "        # Save the predictive model\n",
    "        with open('models/predictive_model.pkl', 'wb') as f:\n",
    "            pickle.dump(predictive_model, f)\n",
    "        \n",
    "        print(\"\\nAll models saved successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving models: {e}\")\n",
    "\n",
    "def test_models(model, vectorizer):\n",
    "    \"\"\"\n",
    "    Tests the sentiment model with sample task descriptions.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained sentiment model\n",
    "    vectorizer: TF-IDF vectorizer\n",
    "    \"\"\"\n",
    "    test_texts = [\n",
    "        \"This task is urgent and needs immediate attention\",\n",
    "        \"I'm looking forward to working on this interesting project\",\n",
    "        \"This is a low priority task that can be done anytime\",\n",
    "        \"This task is frustrating and difficult to complete\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nTesting sentiment analysis on sample task descriptions:\")\n",
    "    for text in test_texts:\n",
    "        sentiment, confidence = predict_sentiment(text, model, vectorizer)\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"Predicted sentiment: {sentiment} (confidence: {confidence:.2f})\\n\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the sentiment analysis model development pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the provided file path\n",
    "        file_path = '/Users/krishnashetty/Desktop/task_manger/Twitter_Data.csv'\n",
    "        print(f\"Using dataset from: {file_path}\")\n",
    "        \n",
    "        # Load and prepare data\n",
    "        X, y = load_and_prepare_data(file_path)\n",
    "        \n",
    "        # Train the sentiment model\n",
    "        model, vectorizer, metrics = train_sentiment_model(X, y)\n",
    "        \n",
    "        # Create task optimization and predictive models\n",
    "        task_optimizer = create_task_optimization_model()\n",
    "        predictive_model = create_predictive_analytics_model()\n",
    "        \n",
    "        # Save all models\n",
    "        save_models(model, vectorizer, task_optimizer, predictive_model)\n",
    "        \n",
    "        # Test the sentiment model\n",
    "        test_models(model, vectorizer)\n",
    "        \n",
    "        print(\"\\nModel development completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e0960-86e7-4602-a1ca-5dd9531c8600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3336185-bfc5-4cbc-a7c9-248a71314e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
